import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix

# ================= é…ç½®è·¯å¾„ =================
# 1. ä½ çš„é¢„æµ‹ç»“æœ (ID, TARGET)
SUBMISSION_PATH = "/your_path/all_results/final_submission.csv"

# 2. ç«èµ›æä¾›çš„æµ‹è¯•é›†åˆ—è¡¨ (ID -> æ–‡ä»¶å çš„æ˜ å°„è¡¨)
ID_MAP_PATH = "Kaggle_Data/metadata/kaggle_test.csv"

# 3. å®˜æ–¹å®Œæ•´æ ‡ç­¾ (æ–‡ä»¶å -> çœŸå®ç±»åˆ« çš„æ˜ å°„è¡¨)
GROUND_TRUTH_PATH = "urbansound8k/UrbanSound8K.csv"

# ================= ä¸»ç¨‹åº =================
def evaluate():
    print(f"Loading submission: {SUBMISSION_PATH}")
    print(f"Loading ID map:     {ID_MAP_PATH}")
    print(f"Loading GroundTruth:{GROUND_TRUTH_PATH}")

    # 1. è¯»å–æ‰€æœ‰æ–‡ä»¶
    try:
        sub_df = pd.read_csv(SUBMISSION_PATH)    # cols: ID, TARGET
        map_df = pd.read_csv(ID_MAP_PATH)        # cols: ID, slice_file_name, ...
        gt_df = pd.read_csv(GROUND_TRUTH_PATH)   # cols: slice_file_name, classID, ...
    except FileNotFoundError as e:
        print(f"\nâŒ Error: æ‰¾ä¸åˆ°æ–‡ä»¶ -> {e}")
        print("è¯·æ£€æŸ¥ `ID_MAP_PATH` ç­‰è·¯å¾„æ˜¯å¦æ­£ç¡®")
        return

    # 2. ç¬¬ä¸€æ­¥åˆå¹¶ï¼šæŠŠé¢„æµ‹ç»“æœ (ID) å’Œ æ–‡ä»¶å å…³è”èµ·æ¥
    try:
        # ç¡®ä¿ ID éƒ½æ˜¯ int ç±»å‹
        sub_df['ID'] = sub_df['ID'].astype(int)
        map_df['ID'] = map_df['ID'].astype(int)
        
        result_df = pd.merge(sub_df, map_df[['ID', 'slice_file_name']], on='ID', how='left')
    except KeyError:
        print("âŒ Error: csv ä¸­æ‰¾ä¸åˆ° 'ID' æˆ– 'slice_file_name' åˆ—")
        return

    # 3. å‡†å¤‡çœŸå®æ ‡ç­¾å­—å…¸ {æ–‡ä»¶å: çœŸå®ç±»åˆ«}
    # åªéœ€è¦ Fold 9 å’Œ 10 (å‡è®¾ Kaggle Test å¯¹åº”çš„æ˜¯åŸå§‹æ•°æ®çš„ fold 9 å’Œ 10)
    gt_df = gt_df[gt_df['fold'].isin([9, 10])]
    gt_dict = dict(zip(gt_df['slice_file_name'], gt_df['classID']))

    # 4. ç¬¬äºŒæ­¥åŒ¹é…ï¼šé€šè¿‡æ–‡ä»¶åè·å–çœŸå®æ ‡ç­¾
    y_true = []
    y_pred = []
    
    missing_count = 0
    
    for _, row in result_df.iterrows():
        fname = row['slice_file_name']
        pred = row['TARGET']
        
        if fname in gt_dict:
            y_true.append(gt_dict[fname])
            y_pred.append(pred)
        else:
            missing_count += 1

    print(f"\nSuccessfully matched {len(y_true)} samples.")
    if missing_count > 0:
        print(f"âš ï¸ Warning: {missing_count} files in submission were not found in Official Fold 9/10.")

    if len(y_true) == 0:
        print("âŒ Error: æ²¡æœ‰æˆåŠŸåŒ¹é…ä»»ä½•æ•°æ®ã€‚")
        return

    # 5. è®¡ç®—æŒ‡æ ‡
    acc = accuracy_score(y_true, y_pred)
    macro_f1 = f1_score(y_true, y_pred, average='macro')
    final_score = 0.8 * acc + 0.2 * macro_f1

    # è·å–ç±»åˆ«åç§° (ç¡®ä¿é¡ºåºæ­£ç¡®)
    # UrbanSound8K çš„ classID é€šå¸¸æ˜¯ 0-9ï¼Œè¿™é‡Œåšä¸€ä¸ªç®€å•çš„æ’åºæå–
    class_map = gt_df[['classID', 'class']].drop_duplicates().sort_values('classID')
    class_names = class_map['class'].tolist()

    # 6. è¾“å‡ºæŠ¥å‘Š
    print("\n" + "="*40)
    print("       ğŸ‰ FINAL EVALUATION ğŸ‰       ")
    print("="*40)
    print(f"âœ… Accuracy  : {acc:.5f}  (80%)")
    print(f"âœ… Macro F1  : {macro_f1:.5f}  (20%)")
    print("-" * 40)
    print(f"ğŸ† SCORE     : {final_score:.5f}")
    print("="*40)

    print("\nDetailed Report:")
    print(classification_report(y_true, y_pred, target_names=class_names, digits=4))

    # ================= æ–°å¢ï¼šç»˜åˆ¶æ··æ·†çŸ©é˜µ =================
    print("Generating Confusion Matrix Plot...")
    
    # è®¡ç®—æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(y_true, y_pred)

    # ç»˜å›¾
    plt.figure(figsize=(12, 10))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=class_names, yticklabels=class_names)
    
    plt.ylabel('True Label', fontsize=12)
    plt.xlabel('Predicted Label', fontsize=12)
    plt.title(f'Confusion Matrix\nAcc: {acc:.4f} | F1: {macro_f1:.4f}', fontsize=14)
    plt.xticks(rotation=45)
    plt.tight_layout()

    # ä¿å­˜å›¾ç‰‡
    save_path = "confusion_matrix.png"
    plt.savefig(save_path, dpi=300)
    print(f"âœ… Confusion Matrix saved to: {save_path}")

if __name__ == "__main__":
    evaluate()